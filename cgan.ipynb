{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cgan.ipynb","provenance":[],"authorship_tag":"ABX9TyN0Mc//nTufuH0T88fovcYx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"IiDl5Vkd-2zJ","executionInfo":{"status":"ok","timestamp":1658326636080,"user_tz":-120,"elapsed":3193,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"outputs":[],"source":["CUDA_LAUNCH_BLOCKING = \"1\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.datasets import IMDB\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.functional import to_map_style_dataset\n","\n","import time\n","import os\n","\n","import numpy as np\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"]},{"cell_type":"code","source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","BATCH_SIZE = 2\n","MIN_FREQ = 10\n","\n","UNK_IDX = 0\n","BOS_IDX = 1\n","EOS_IDX = 2\n","PAD_IDX = 3\n","SPEC_TOKENS = ['<UNK>', '<BOS>', '<EOS>', '<PAD>']\n","\n","lr = 1e-3\n","lr_decay_every = 1000000\n","epochs = 10\n","log_interval = 10"],"metadata":{"id":"OK4OjQkiGa_t","executionInfo":{"status":"ok","timestamp":1658326639080,"user_tz":-120,"elapsed":3,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class WordDataset:\n","    def __init__(self):\n","        self.tokenizer = get_tokenizer('basic_english')\n","\n","        train_dataset, test_dataset = iter(IMDB(split=('train', 'test')))\n","        train_dataset, test_dataset = to_map_style_dataset(train_dataset), to_map_style_dataset(test_dataset)\n","\n","        self.vocab = build_vocab_from_iterator(self.build_vocab([train_dataset, test_dataset]), specials=SPEC_TOKENS)\n","        self.vocab.set_default_index(self.vocab['<UNK>'])\n","        self.vocab_length = len(self.vocab.get_itos())\n","\n","        self.vectorizer = CountVectorizer(vocabulary=self.vocab.get_itos(), tokenizer=self.tokenizer)\n","\n","        self.text_transform = lambda x: self.vocab(self.tokenizer(x))\n","\n","        self.train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=self.vectorize_batch)\n","        self.test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=self.vectorize_batch)\n","\n","    def build_vocab(self, datasets):\n","        for dataset in datasets:\n","            for _, text in dataset:\n","                yield self.tokenizer(text)\n","\n","    def vectorize_batch(self, batch):\n","        label_list, text_list, offsets = [], [], []\n","        for Y, X in batch:\n","          label_list.append(self.text_transform(Y))\n","          tmp_X = torch.tensor(self.text_transform(X), dtype=torch.int64)\n","          text_list.append(torch.cat([torch.tensor([BOS_IDX]), tmp_X, torch.tensor([EOS_IDX])]))\n","        label_list = torch.tensor(label_list, dtype=torch.int64)\n","        text_list = torch.cat(text_list)\n","        return label_list.to(DEVICE), text_list.to(DEVICE)\n","\n","    def text_translate(self, x):\n","        return ' '.join([self.vocab.get_itos()[i] for i in x])"],"metadata":{"id":"koUkrw2eGYGT","executionInfo":{"status":"ok","timestamp":1658326640806,"user_tz":-120,"elapsed":3,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Generator(nn.Module):\n","  def __init__(self, emb_dim, h_dim, vocab_len, max_seq_len):\n","        super(Generator, self).__init__()\n","\n","        self.h_dim = h_dim\n","        self.emb_dim = emb_dim\n","        self.max_seq_len = max_seq_len\n","\n","        # Word embeddings layer\n","        self.embedding = nn.Embedding(vocab_len, emb_dim, padding_idx=PAD_IDX)\n","\n","        # Generator\n","        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=True)\n","        self.out = nn.Linear(h_dim, vocab_len)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","  def forward(self, text, hidden):\n","        emb = self.embedding(text)\n","        if len(text.size()) == 1:\n","            emb = emb.unsqueeze(1)\n","\n","        out, hidden = self.lstm(emb, hidden)\n","        out = out.contiguous().view(-1, self.h_dim)\n","        out = self.out(out)\n","        \n","        pred = self.softmax(out)\n","        \n","        return pred"],"metadata":{"id":"7XuYYMBayPKw","executionInfo":{"status":"ok","timestamp":1658327663649,"user_tz":-120,"elapsed":321,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","  def __init__(self, h_dim, c_dim, emb_dim, vocab_len, dropout=0.3):\n","    super(Discriminator, self).__init__()\n","    \n","    self.h_dim = h_dim\n","    self.emb_dim = emb_dim\n","\n","    # Word embeddings layer\n","    self.embedding = nn.Embedding(vocab_len, emb_dim, padding_idx=PAD_IDX)\n","\n","    # Disriminator\n","    self.gru = nn.GRU(emb_dim, h_dim, num_layers=2, bidirectional=True, dropout=dropout)\n","    self.hidden = nn.Linear(4 * h_dim, c_dim)\n","    self.out = nn.Linear(c_dim, 2)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, inputs):\n","    label, text = inputs\n","\n","    label_output = self.embedding(label)\n","    text_output = self.embedding(text)\n","    \n","    concat = torch.cat((text_output, label_output), dim=0)\n","\n","    hidden = torch.zeros(4, concat.size(0), self.h_dim).to(DEVICE)\n","\n","    _, hidden = self.gru(concat.unsqueeze(0), hidden)\n","    hidden = hidden.permute(1, 0, 2).contiguous()\n","\n","    out = self.hidden(hidden.view(-1, 4 * self.h_dim))\n","\n","    feature = torch.tanh(out)\n","\n","    pred = self.out(self.dropout(feature))\n","    \n","    # pred = self.model(concat)\n","    \n","    return pred\n"],"metadata":{"id":"TXbHWSOLIHsa","executionInfo":{"status":"ok","timestamp":1658326646563,"user_tz":-120,"elapsed":319,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["dataset = WordDataset()"],"metadata":{"id":"18fUaaBfm_yT","executionInfo":{"status":"ok","timestamp":1658326713243,"user_tz":-120,"elapsed":64775,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b19ed03c-cce6-4e3a-f1f5-b73d106c27ef"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 84.1M/84.1M [00:24<00:00, 3.45MB/s]\n"]}]},{"cell_type":"markdown","source":["=== TRAINING ==="],"metadata":{"id":"t9jveSqyEaJQ"}},{"cell_type":"code","source":["discriminator = Discriminator(\n","    vocab_len=dataset.vocab_length,\n","    emb_dim=64,\n","    h_dim=64,\n","    c_dim=2\n",").to(DEVICE)\n","\n","generator = Generator(\n","    vocab_len=dataset.vocab_length,\n","    max_seq_len=15,\n","    emb_dim=64,\n","    h_dim=64\n",").to(DEVICE)\n","\n","generator_optimizer = optim.Adam(generator.parameters(), lr=lr)\n","discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n","\n","for e in range(epochs):\n","    discriminator.train()\n","    generator.train()\n","\n","    discriminator_loss_list = []\n","    generator_loss_list = []\n","\n","    for idx, (labels, texts) in enumerate(dataset.train_loader):\n","      print('L:', labels.shape)\n","      print('T:', texts.shape)\n","\n","      real_label = torch.ones(texts.size(0) + BATCH_SIZE, 2).to(DEVICE)\n","      fake_label = torch.zeros(texts.size(0) + BATCH_SIZE, 2).to(DEVICE)\n","\n","      noise = torch.randn([texts.size(0) + BATCH_SIZE, 100]).to(DEVICE)\n","      conditional = torch.randint(6, 7, (texts.size(0) + BATCH_SIZE,)).to(DEVICE)\n","\n","      # ==== FAKE ====\n","      discriminator_optimizer.zero_grad()\n","\n","      discriminator_real_loss = F.binary_cross_entropy(discriminator((labels.view(-1), texts)), real_label)\n","      \n","      noise_vector = torch.randn(texts.size(0), 64, device=DEVICE)\n","      noise_vector = noise_vector.to(DEVICE)\n","\n","      generated_text = generator((noise_vector, labels))\n","\n","      output = discriminator((generated_text.detach(), labels))\n","      \n","      discriminator_fake_loss = F.binary_cross_entropy(output,  fake_label)\n","\n","      discriminator_total_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n","      \n","      discriminator_loss_list.append(discriminator_total_loss)\n","      \n","      discriminator_total_loss.backward()\n","      \n","      discriminator_optimizer.step()\n","\n","      # ==== REAL ====\n","      generator_optimizer.zero_grad()\n","      \n","      generator_loss = F.binary_cross_entropy(discriminator((generated_text, labels)), real_label)\n","      \n","      generator_loss_list.append(generator_loss)\n","      \n","      generator_loss.backward()\n","      \n","      generator_optimizer.step()"],"metadata":{"id":"BO6orOupEZ6u","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1658327669552,"user_tz":-120,"elapsed":629,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}},"outputId":"1826038b-69df-42f9-f048-7d604968f848"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["L: torch.Size([2, 1])\n","T: torch.Size([575])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5fd031e565e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mdiscriminator_real_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mnoise_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2915\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"]}]},{"cell_type":"code","source":["!pip install torchtext==0.11.0"],"metadata":{"id":"-ni5wsD7nVdM"},"execution_count":null,"outputs":[]}]}