{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentiGAN.ipynb","provenance":[],"authorship_tag":"ABX9TyPQjUg18M7zo4yazEiIw+GZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"T0jNgQO4OkE3","executionInfo":{"status":"ok","timestamp":1658331163392,"user_tz":-120,"elapsed":2415,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from itertools import chain"]},{"cell_type":"code","source":["BATCH_SIZE = 8\n","\n","lr = 0.01"],"metadata":{"id":"oAXbP4PsPd3j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Generator(object):\n","    def __init__(self, num_emb, vocab_dict, emb_dim, num_units, max_seq_len, reward_gamma=0.95):\n","        self.num_emb = num_emb\n","        self.vocab_dict = vocab_dict\n","        self.emb_dim = emb_dim\n","        self.num_units = num_units\n","        self.max_sequence_length = max_seq_len\n","        self.reward_gamma = reward_gamma\n","        self.grad_clip = 5.0\n","        self.keep_prob = 1.0\n","\n","        self.g_embeddings = tf.Variable(self.init_matrix([self.num_emb, self.emb_dim]))\n","\n","        with tf.variable_scope('placeholder'):\n","            self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.max_sequence_length])\n","            self.sequence_lengths = tf.placeholder(tf.int32, shape=[self.batch_size])\n","\n","        with tf.variable_scope('embedding'):\n","            # batch_size major\n","            self.emb_x = tf.nn.embedding_lookup(self.g_embeddings, self.x)\n","\n","        with tf.variable_scope('projection'):\n","            self.output_layer = layers_core.Dense(self.num_emb, use_bias=False)\n","\n","        def _get_cell(_num_units):\n","            return tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(_num_units),\n","                                                 input_keep_prob=self.keep_prob)\n","\n","        with tf.variable_scope(\"decoder\"):\n","            self.decoder_cell = _get_cell(self.num_units)\n","            \n","            self.c = tf.random_normal([self.batch_size, self.num_units], mean=0, stddev=4)\n","            self.h = tf.random_normal([self.batch_size, self.num_units], mean=0, stddev=4)\n","\n","            self.initial_state = tf.contrib.rnn.LSTMStateTuple(c=self.c, h=self.h)\n","\n","            ###################### pretain with targets ######################\n","            helper_pt = tf.contrib.seq2seq.TrainingHelper(\n","                inputs=self.emb_x,\n","                sequence_length=self.sequence_lengths,\n","                time_major=False,\n","            )\n","            decoder_pt = tf.contrib.seq2seq.BasicDecoder(\n","                cell=self.decoder_cell,\n","                helper=helper_pt,\n","                initial_state=self.initial_state,\n","                output_layer=self.output_layer\n","            )\n","\n","            outputs_pt, _final_state, sequence_lengths_pt = tf.contrib.seq2seq.dynamic_decode(\n","                decoder=decoder_pt,\n","                output_time_major=False,\n","                maximum_iterations=self.max_sequence_length,\n","                swap_memory=True,\n","            )\n","            self.logits_pt = outputs_pt.rnn_output\n","\n","            self.g_predictions = tf.nn.softmax(self.logits_pt)\n","\n","            self.targets = tf.placeholder(dtype=tf.int32, shape=[None, None])\n","            self.target_weights = tf.placeholder(dtype=tf.float32, shape=[None, None])\n","\n","            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits_pt)\n","            self.pretrain_loss = tf.reduce_sum(crossent * self.target_weights) / tf.to_float(self.batch_size)\n","\n","            self.global_step = tf.Variable(0, trainable=False)\n","            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n","            gradients, v = zip(*optimizer.compute_gradients(self.pretrain_loss))\n","            gradients, _ = tf.clip_by_global_norm(gradients, self.grad_clip)\n","            self.pretrain_updates = optimizer.apply_gradients(zip(gradients, v), global_step=self.global_step)\n","\n","            ################## gan loss with rewards  #####################\n","            self.rewards = tf.placeholder(dtype=tf.float32, shape=[None, None])\n","            self.rewards_loss = tf.reduce_sum(\n","                tf.reduce_sum(\n","                    tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.clip_by_value(\n","                        tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n","                    , 1) * tf.reshape(self.rewards, [-1])  # * tf.reshape(self.target_weights, [-1])\n","            )\n","            optimizer_gan = tf.train.RMSPropOptimizer(self.learning_rate)\n","            gradients_gan, v_gan = zip(*optimizer_gan.compute_gradients(self.rewards_loss))\n","            gradients_gan, _gan = tf.clip_by_global_norm(gradients_gan, self.grad_clip)\n","            self.rewards_updates = optimizer_gan.apply_gradients(zip(gradients_gan, v_gan), global_step=self.global_step)\n","\n","\n","            ###################### train without targets ######################\n","            helper_o = tf.contrib.seq2seq.SampleEmbeddingHelper(\n","                self.g_embeddings,\n","                tf.fill([self.batch_size], self.vocab_dict['<GO>']),\n","                end_token=self.vocab_dict['<EOS>']\n","            )\n","            decoder_o = tf.contrib.seq2seq.BasicDecoder(\n","                cell=self.decoder_cell,\n","                helper=helper_o,\n","                initial_state=self.initial_state,\n","                output_layer=self.output_layer\n","            )\n","            outputs_o, _final_state_o, sequence_lengths_o = tf.contrib.seq2seq.dynamic_decode(\n","                decoder=decoder_o,\n","                output_time_major=False,\n","                maximum_iterations=self.max_sequence_length,\n","                swap_memory=True,\n","            )\n","\n","            self.out_lenghts = sequence_lengths_o\n","            self.out_tokens = tf.unstack(outputs_o.sample_id, axis=0)\n","\n","            ######################  infer  ######################\n","            helper_i = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n","                self.g_embeddings,\n","                tf.fill([self.batch_size], self.vocab_dict['<GO>']),\n","                end_token=self.vocab_dict['<EOS>']\n","            )\n","            decoder_i = tf.contrib.seq2seq.BasicDecoder(\n","                cell=self.decoder_cell,\n","                helper=helper_i,\n","                initial_state=self.initial_state,\n","                output_layer=self.output_layer\n","            )\n","\n","            outputs_i, _final_state_i, sequence_lengths_i = tf.contrib.seq2seq.dynamic_decode(\n","                decoder=decoder_i,\n","                output_time_major=False,\n","                maximum_iterations=self.max_sequence_length,\n","                swap_memory=True,\n","            )\n","\n","            sample_id = outputs_i.sample_id\n","\n","            self.infer_tokens = tf.unstack(sample_id, axis=0)\n","\n","            self.rollout_input_ids = tf.placeholder(dtype=tf.int32, shape=[None, None])\n","            self.rollout_input_length = tf.placeholder(dtype=tf.int32, shape=())\n","            self.rollout_input_lengths = tf.placeholder(dtype=tf.int32, shape=[None])\n","            self.rollout_next_id = tf.placeholder(dtype=tf.int32, shape=[None])\n","\n","            rollout_inputs = tf.nn.embedding_lookup(self.g_embeddings, self.rollout_input_ids)\n","            helper_ro = tf.contrib.seq2seq.TrainingHelper(\n","                rollout_inputs,\n","                self.rollout_input_lengths\n","            )\n","            rollout_decoder = tf.contrib.seq2seq.BasicDecoder(\n","                cell=self.decoder_cell,\n","                helper=helper_ro,\n","                initial_state=self.initial_state,\n","                output_layer=self.output_layer\n","            )\n","            _, final_state_ro, _ = tf.contrib.seq2seq.dynamic_decode(\n","                rollout_decoder,\n","                maximum_iterations=self.max_sequence_length,\n","                swap_memory=True\n","            )\n","            initial_state_MC = final_state_ro\n","            helper_MC = tf.contrib.seq2seq.SampleEmbeddingHelper(\n","                self.g_embeddings,\n","                self.rollout_next_id,\n","                end_token=self.vocab_dict['<EOS>']\n","            )\n","            rollout_decoder_MC = tf.contrib.seq2seq.BasicDecoder(\n","                cell=self.decoder_cell,\n","                helper=helper_MC,\n","                initial_state=initial_state_MC,\n","                output_layer=self.output_layer\n","            )\n","            self.max_mc_length = tf.cast(self.max_sequence_length - self.rollout_input_length, tf.int32)\n","            decoder_output_MC, _, _ = tf.contrib.seq2seq.dynamic_decode(\n","                rollout_decoder_MC,\n","                output_time_major=False,\n","                maximum_iterations=self.max_mc_length,\n","                swap_memory=True\n","            )\n","            self.sample_id_MC = decoder_output_MC.sample_id\n","\n","        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n","\n","    def pretrain_step(self, sess, x):\n","        input_x, lengths_x = self.pad_input_data(x)\n","        target_x = self.pad_target_data(x)\n","        target_weights = self.get_weights(lengths_x)\n","\n","        outputs = sess.run([self.pretrain_updates, self.pretrain_loss], feed_dict={\n","            self.x: input_x,\n","            self.sequence_lengths: [self.max_sequence_length] * self.batch_size,\n","            self.targets: target_x,\n","            self.target_weights: target_weights\n","        })\n","        return outputs\n","\n","    def update_with_rewards(self, sess, x, rewards):\n","        input_x, lengths_x = self.pad_input_data(x)\n","        target_x = self.pad_target_data(x)\n","        target_weights = self.get_weights(lengths_x)\n","\n","        [rewards_updates, rewards_loss] = sess.run([self.rewards_updates, self.rewards_loss], feed_dict={\n","            self.x: input_x,\n","            self.sequence_lengths: [self.max_sequence_length] * self.batch_size,\n","            self.targets: target_x,\n","            self.rewards: rewards,\n","            self.target_weights: target_weights\n","        })\n","        return rewards_loss\n","\n","    def generate(self, sess):\n","        [outputs] = sess.run([self.out_tokens])\n","        return outputs  # , out_lenghts\n","\n","    def infer(self, sess):\n","        [outputs] = sess.run([self.infer_tokens])\n","        return outputs\n","\n","    def init_matrix(self, shape):\n","        return tf.random_normal(shape, stddev=0.1)\n","\n","    def _get_cell(self, num_units):\n","        return tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(num_units),\n","                                             input_keep_prob=self.keep_prob)\n","\n","    def pad_input_data(self, x):\n","        max_l = self.max_sequence_length\n","        go_id = self.vocab_dict['<GO>']\n","        end_id = self.vocab_dict['<EOS>']\n","        x_len = len(x)\n","        ans = np.zeros((x_len, max_l), dtype=int)\n","        ans_lengths = []\n","        for i in range(x_len):\n","            ans[i][0] = go_id\n","            jj = min(len(x[i]), self.max_sequence_length - 2)\n","            for j in range(jj):\n","                ans[i][j + 1] = x[i][j]\n","            ans[i][jj+1] = end_id\n","            ans_lengths.append(jj + 2)\n","        return ans, ans_lengths\n","\n","    def pad_target_data(self, x):\n","        max_l = self.max_sequence_length\n","        end_id = self.vocab_dict['<EOS>']\n","        x_len = len(x)\n","        ans = np.zeros((x_len, max_l), dtype=int)\n","        for i in range(x_len):\n","            jj = min(len(x[i]), max_l-1)\n","            for j in range(jj):\n","                ans[i][j] = x[i][j]\n","            ans[i][jj] = end_id\n","        return ans\n","\n","    def delete_output_data(self, x, lengths):\n","        ans = []\n","        for i, item in enumerate(x):\n","            ans.append(item[:lengths[i]])\n","        return np.array(ans)\n","\n","    def get_weights(self, lengths):\n","        x_len = len(lengths)\n","        max_l = self.max_sequence_length\n","        ans = np.zeros((x_len, max_l))\n","        for ll in range(x_len):\n","            kk = lengths[ll] - 1\n","            for jj in range(kk):\n","                ans[ll][jj] = 1/float(kk)\n","        return ans\n","\n","    def get_reward(self, sess, input_x, rollout_num, discriminator):\n","        x, lengths_x = self.pad_input_data(input_x)\n","        input_x = self.padding(input_x, self.max_sequence_length)\n","        rewards = []\n","\n","        for i in range(rollout_num):\n","            for given_num in range(1, self.max_sequence_length):\n","\n","                rollout_next_id = []\n","                for _item in x:\n","                    rollout_next_id.append(_item[given_num])\n","\n","                feed = {\n","                    self.rollout_input_ids: x,\n","                    self.rollout_input_length: given_num,\n","                    self.rollout_input_lengths: [given_num] * self.batch_size,\n","                    self.rollout_next_id: rollout_next_id\n","                }\n","\n","                mc_samples = sess.run(self.sample_id_MC, feed)\n","                \n","                fix_samples = np.array(input_x)[:, 0: given_num]\n","\n","                samples = np.concatenate((fix_samples, mc_samples), axis=1)\n","                samples = self.padding(samples, self.max_sequence_length)\n","\n","                feed = {discriminator.input_x: samples, discriminator.dropout_keep_prob: 1.0}\n","                ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n","                ypred = np.array([item[0] for item in ypred_for_auc])\n","                if i == 0:\n","                    rewards.append(ypred)\n","                else:\n","                    rewards[given_num - 1] += ypred\n","\n","            # the last token reward\n","            feed = {discriminator.input_x: input_x, discriminator.dropout_keep_prob: 1.0}\n","            ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n","            ypred = np.array([item[0] for item in ypred_for_auc])\n","            if i == 0:\n","                rewards.append(ypred)\n","            else:\n","                rewards[self.max_sequence_length - 1] += ypred\n","\n","        rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)  # batch_size x seq_length\n","        rewards = self.get_new_rewards(lengths_x, rewards)\n","        return rewards\n","\n","    def get_new_rewards(self, lengths_x, rewards):\n","        r = len(rewards[0])\n","        for i in range(len(lengths_x)):\n","            l = lengths_x[i]\n","            for j in range(l, r):\n","                rewards[i][j] = rewards[i][-1]\n","        return rewards\n","\n","    def padding(self, inputs, max_sequence_length):\n","        batch_size = len(inputs)\n","        inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32)  # == PAD\n","        for i, seq in enumerate(inputs):\n","            for j, element in enumerate(seq):\n","                inputs_batch_major[i, j] = element\n","        return inputs_batch_major\n","\n","    def save_model(self, sess):\n","        self.saver.save(sess, 'save/ckpt/model.ckpt')\n","        print(\"save model success!\")"],"metadata":{"id":"3eIx8nrKPY2w"},"execution_count":null,"outputs":[]}]}