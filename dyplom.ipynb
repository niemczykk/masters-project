{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"IiDl5Vkd-2zJ","executionInfo":{"status":"ok","timestamp":1660678428148,"user_tz":-120,"elapsed":4761,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.datasets import IMDB\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.functional import to_map_style_dataset\n","\n","import time\n","import os\n","\n","import numpy as np\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from google.colab import drive"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OK4OjQkiGa_t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660678447102,"user_tz":-120,"elapsed":17319,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}},"outputId":"ce542116-d7e7-452c-f8af-596a0f462fa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive')\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","BATCH_SIZE = 2\n","MIN_FREQ = 10\n","\n","UNK_IDX = 0\n","BOS_IDX = 1\n","EOS_IDX = 2\n","PAD_IDX = 3\n","SPEC_TOKENS = ['<UNK>', '<BOS>', '<EOS>', '<PAD>']\n","\n","lr = 1e-3\n","lr_decay_every = 1000000\n","epochs = 5\n","log_interval = 10"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"koUkrw2eGYGT","executionInfo":{"status":"ok","timestamp":1660678451159,"user_tz":-120,"elapsed":303,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"outputs":[],"source":["class WordDataset:\n","    def __init__(self):\n","        self.tokenizer = get_tokenizer('basic_english')\n","\n","        train_dataset, test_dataset = iter(IMDB(split=('train', 'test')))\n","        train_dataset, test_dataset = to_map_style_dataset(train_dataset), to_map_style_dataset(test_dataset)\n","\n","        self.vocab = build_vocab_from_iterator(self.build_vocab([train_dataset, test_dataset]), specials=SPEC_TOKENS)\n","        self.vocab.set_default_index(self.vocab['<UNK>'])\n","        self.vocab_length = len(self.vocab.get_itos())\n","\n","        self.vectorizer = CountVectorizer(vocabulary=self.vocab.get_itos(), tokenizer=self.tokenizer)\n","\n","        self.text_transform = lambda x: self.vocab(self.tokenizer(x))\n","\n","        self.train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=self.vectorize_batch)\n","        self.test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=self.vectorize_batch)\n","\n","    def build_vocab(self, datasets):\n","        for dataset in datasets:\n","            for _, text in dataset:\n","                yield self.tokenizer(text)\n","\n","    def vectorize_batch(self, batch):\n","        label_list, text_list, offsets = [], [], []\n","        for Y, X in batch:\n","            print(Y, X)\n","            label_list.append(self.text_transform(Y))\n","            tmp_X = torch.tensor(self.text_transform(X), dtype=torch.int64)\n","            text_list.append(torch.cat([torch.tensor([BOS_IDX]), tmp_X, torch.tensor([EOS_IDX])]))\n","        label_list = torch.tensor(label_list, dtype=torch.int64)\n","        text_list = torch.cat(text_list)\n","        return label_list.to(DEVICE), text_list.to(DEVICE)\n","\n","    def text_translate(self, x):\n","        return ' '.join([self.vocab.get_itos()[i] for i in x])\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"81eCGBzSGswJ","executionInfo":{"status":"ok","timestamp":1660678454026,"user_tz":-120,"elapsed":287,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}}},"outputs":[],"source":["class cVAE(nn.Module):\n","    def __init__(self, vocab_len, h_dim, z_dim, c_dim, p_word_dropout=0.3, max_sent_len=15):\n","        super(cVAE, self).__init__()\n","\n","        self.MAX_SENT_LEN = max_sent_len\n","\n","        self.vocab_len = vocab_len\n","        self.h_dim = h_dim\n","        self.z_dim = z_dim\n","        self.c_dim = c_dim\n","        self.p_word_dropout = p_word_dropout\n","\n","        # Word embeddings layer\n","        self.emb_dim = h_dim\n","        self.word_emb = nn.Embedding(vocab_len, h_dim, PAD_IDX)\n","\n","        # Encoder\n","        self.encoder = nn.GRU(self.emb_dim, h_dim)\n","        self.mu = nn.Linear(h_dim, z_dim)\n","        self.log_var = nn.Linear(h_dim, z_dim)\n","\n","        # Decoder\n","        self.decoder = nn.GRU(self.emb_dim + z_dim + c_dim, z_dim + c_dim, dropout=0.3)\n","        self.decoder_fc = nn.Linear(z_dim + c_dim, vocab_len)\n","\n","    def forward(self, sentence, label):\n","        self.train()\n","\n","        encoder_inputs = sentence\n","        decoder_inputs = sentence\n","        decoder_targets = torch.cat([sentence[1:], torch.tensor([PAD_IDX]).to(DEVICE)], dim=0).to(DEVICE)\n","        \n","        mu, log_var = self.encoder_fn(encoder_inputs)\n","        z = self.reparameterize(mu, log_var)\n","        \n","        y = self.decoder_fn(decoder_inputs, z, label)\n","\n","        recon_loss = F.cross_entropy(\n","            y.view(-1, self.vocab_len),\n","            decoder_targets.view(-1),\n","            size_average=True\n","        )\n","\n","        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(log_var) + mu ** 2 - 1 - log_var, 1))\n","\n","        return recon_loss, kl_loss\n","\n","    def encoder_fn(self, inputs):\n","        inputs = self.word_emb(inputs).unsqueeze(0)\n","\n","        _, h = self.encoder(inputs, None)\n","        h = h.view(-1, self.h_dim)\n","\n","        mu = self.mu(h)\n","        log_var = self.log_var(h)\n","\n","        return mu, log_var\n","\n","    def decoder_fn(self, inputs, z, label):\n","        decoder_inputs = self.word_dropout(inputs)\n","\n","        seq_len = decoder_inputs.size(0)\n","        \n","        init_h = torch.cat([z.unsqueeze(0), torch.transpose(label, 0, 1).expand(seq_len, -1).unsqueeze(0)], dim=2)\n","\n","        inputs_emb = self.word_emb(decoder_inputs).unsqueeze(0)\n","        inputs_emb = torch.cat([inputs_emb, init_h], dim=2).to(DEVICE)\n","\n","        outputs, a = self.decoder(inputs_emb, init_h)\n","        _, seq_len, sentence_size = outputs.size()\n","\n","        y = self.decoder_fc(outputs)\n","\n","        return y\n","\n","    def word_dropout(self, inputs):\n","        data = inputs.clone()\n","\n","        mask = torch.from_numpy(np.random.binomial(1, p=self.p_word_dropout, size=tuple(data.size())).astype('bool'))\n","\n","        data[mask] = UNK_IDX\n","\n","        return data\n","\n","    def reparameterize(self, mu, log_var):\n","        eps = torch.randn(self.z_dim).to(DEVICE)\n","        return mu + torch.exp(log_var / 2).to(DEVICE) * eps\n","\n","    def sample_z(self, sentence_size):\n","        z = torch.randn(sentence_size, self.z_dim).to(DEVICE)\n","        return z\n","\n","    def sample_sentence(self, z, c, temp=1):\n","        self.eval()\n","\n","        word = torch.LongTensor([BOS_IDX]).to(DEVICE)\n","\n","        z = z.view(1, 1, -1)\n","        c = c.view(1, 1, -1)\n","\n","        h = torch.cat([z, c], dim=2).to(DEVICE)\n","\n","        outputs = []\n","\n","        for i in range(self.MAX_SENT_LEN):\n","            emb = self.word_emb(word).view(1, 1, -1)\n","            emb = torch.cat([emb, z, c], dim=2)\n","\n","            output, h = self.decoder(emb, h)\n","            y = self.decoder_fc(output).view(-1)\n","            y = F.softmax(y / temp, dim=0)\n","\n","            idx = torch.multinomial(y, 1)\n","\n","            word = torch.LongTensor([int(idx)]).to(DEVICE)\n","\n","            outputs.append(idx)\n","\n","        self.train()\n","\n","        outputs = torch.LongTensor(outputs)\n","        return outputs\n","\n","    def generate_sentences(self, seed, label):\n","        s = time.time()\n","        z = self.word_emb(seed)\n","        c = torch.FloatTensor([1, 0]).to(DEVICE) if label == 'pos' else torch.FloatTensor([1, 0]).to(DEVICE)\n","        X_gen = self.sample_sentence(z, c)\n","        print('TIME: ', time.time() - s)\n","\n","        return X_gen"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"18fUaaBfm_yT","executionInfo":{"status":"ok","timestamp":1660678510456,"user_tz":-120,"elapsed":53353,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7eebaf72-af69-48d4-ae0f-eea26e79eebc"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 84.1M/84.1M [00:02<00:00, 32.0MB/s]\n"]}],"source":["dataset = WordDataset()"]},{"cell_type":"code","source":["model = cVAE(\n","    vocab_len=dataset.vocab_length,\n","    h_dim=64,\n","    z_dim=64,\n","    c_dim=2\n",").to(DEVICE)\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss(reduction='mean')\n","scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n","total_accu = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WbIb2Bm_OO5","executionInfo":{"status":"ok","timestamp":1660678514176,"user_tz":-120,"elapsed":282,"user":{"displayName":"Krzysztof Niemczyk","userId":"03291427652623611403"}},"outputId":"10046113-d373-43fd-b869-797567ba39e5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"markdown","metadata":{"id":"oWI78zvg5LWN"},"source":["=== Training script ==="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_i-eO77cZMMb"},"outputs":[],"source":["kld_start_inc = 3000\n","kld_weight = 0.01\n","kld_max = 0.15\n","kld_inc = (kld_max - kld_weight) / (epochs - kld_start_inc)\n","\n","model = cVAE(\n","    vocab_len=dataset.vocab_length,\n","    h_dim=64,\n","    z_dim=64,\n","    c_dim=2\n",").to(DEVICE)\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss(reduction='mean')\n","scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n","total_accu = None\n","\n","for e in range(epochs):\n","    print('Current epoch: ', e)\n","    for idx, (label, text) in enumerate(dataset.train_loader):\n","        recon_loss, kl_loss = model.forward(text, label)\n","        loss = recon_loss + kld_weight * kl_loss\n","        \n","        if e > kld_start_inc and kld_weight < kld_max:\n","            kld_weight += kld_inc\n"," \n","        loss.backward()\n","        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), 5)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        if e % log_interval == 0:\n","            z = model.sample_z(1)\n","\n","            sample_idxs = model.sample_sentence(z, label)\n","            sample_sent = dataset.text_translate(sample_idxs)\n","\n","            print('Iter-{}\\n\\tLoss:\\t\\t{:.4f}\\n\\tRecon:\\t\\t{:.4f}\\n\\tKL:\\t\\t{:.4f}\\n\\tGrad_norm:\\t{:.4f}'\n","                  .format(e, loss.data, recon_loss.data, kl_loss.data, grad_norm))\n","\n","            print('Sample: \"{}\"\\n'.format(sample_sent))\n","\n","torch.save(model.state_dict(), '/content/gdrive/My Drive/DYPLOM/models/cVAE.bin')"]},{"cell_type":"markdown","metadata":{"id":"7Ivxz6FN5Ylv"},"source":["=== Testing script ==="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pG6L1dUW5cUR"},"outputs":[],"source":["torch.manual_seed(int(time.time()))\n","\n","MODEL_NAME = 'cVAE_50'\n","samples = 5\n","seed = torch.Tensor(dataset.text_transform('The')).to(torch.int64).to(DEVICE)\n","\n","model = cVAE(\n","    vocab_len=dataset.vocab_length,\n","    h_dim=64,\n","    z_dim=64,\n","    c_dim=2\n",").to(DEVICE)\n","\n","model.load_state_dict(torch.load('/content/gdrive/My Drive/DYPLOM/models/{}.bin'.format(MODEL_NAME), \n","                                 map_location=lambda storage, \n","                                 loc: storage))\n","for i in range(samples):\n","  print('\\n=== ', i + 1, ' ===')\n","  pred_pos = model.generate_sentences(seed, 'pos')\n","  print('POS: ', dataset.text_translate(pred_pos))\n","\n","  pred_neg = model.generate_sentences(seed, 'neg')\n","  print('NEG: ', dataset.text_translate(pred_neg))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ni5wsD7nVdM"},"outputs":[],"source":["!pip install torchtext==0.11.0"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"waueDIe7qNwg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"W04N_263230_2022_praca_magisterska_zal1.ipynb","provenance":[],"authorship_tag":"ABX9TyMJdbWlV01hdRZbZcUyyuie"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}